from pathlib import Path
import os

configfile: 'example_config.yaml'

email = config['email']
input_assembly = config['genome']
output_dir = Path(config['output'])
results = output_dir / Path(config['results_dir'])

scripts = Path(config['scripts'])
envs = Path(config['envs'])
logs = Path(config['logs'])
profiles = Path(config['hmms'])

operonmapper_output = output_dir / config["operonmapper_output"]
bakta_output = output_dir / config["bakta_output"]
isescan_output = output_dir / config["isescan_output"]
hmm_results = output_dir / config['hmmer_output']
blast_results = output_dir / config['blastp_output']

os.makedirs(hmm_results, exist_ok=True)
os.makedirs(blast_results, exist_ok=True)
os.makedirs(logs, exist_ok=True)
os.makedirs(results, exist_ok=True)
os.makedirs(results / 'images', exist_ok=True)

input_prefix = '.'.join(os.path.basename(config['genome']).split('.')[:-1])
input_dir =  os.path.basename(os.path.dirname(input_assembly))
reference_gff = output_dir / f'{input_prefix}.gff'  # TODO: MAKE IT OPTIONAL
no_transposone_assembly = output_dir / f"{input_prefix}.no_transposone.fna"

with open(input_assembly,'r') as asf:
    seq_id = asf.readline()[1:].strip().split()[0]

no_transposone_rebuild_file = f'{no_transposone_assembly}_{seq_id}.rebuild.csv'

rule all:
    input:
        results / 'operons_annotation.gff3',
        results / 'images' / 'finished.txt',
        # expand(results / 'images/{file}', file=os.listdir(results / 'images')),
        # results / 'operons_annotation_with_blast.gff'
        
rule gene_boundaries:
    input:
        input_assembly
    output:
        output_dir / "prodigal.trn"
    conda:
        envs /"prodigal.yaml"
    shell:
        "prodigal -i {input} -t {output}"

rule annotate_genomes:
    input:
        assembly=input_assembly,
        prodigal_tf = output_dir / "prodigal.trn"
    output:
        bakta_output / f'{input_prefix}.gff3'
    params:
        out=bakta_output,
        db=config['bakta_db']
    conda:
        envs / "bakta.yaml"
    shell:
        """
        ./bakta_loaddb.sh
        bakta {input.assembly} \
            --force \
            --db {params.db} \
            --prodigal-tf {input.prodigal_tf} \
            --output {params.out}
        """

rule find_transposones:
    input:
        fna = input_assembly
    output:
        isescan_output / input_dir / f'{input_prefix}.fna.gff'
    params:
        out=isescan_output
    conda:
        envs / 'isescan.yml'
    threads:
        config['maxthreads']
    shell:
        """
        isescan.py --seqfile {input.fna} --output {params.out} --nthread {threads}
        """

rule cut_transposones:
    input:
        assembly=input_assembly,
        transposon_annotation = isescan_output / input_dir / f'{input_prefix}.fna.gff'
    output:
        nt=no_transposone_assembly,
        nt_rebuild=no_transposone_rebuild_file
    conda:
        envs / 'pythonic.yml'
    shell:
        "python {scripts}/transposon.py cut {input.assembly} {input.transposon_annotation} -o {output.nt}"


rule operon_mapping:
    input:
        no_transposone_assembly
    output:
        pps = operonmapper_output / 'predicted_protein_sequences',
        loo = operonmapper_output / 'list_of_operons'
    params:
        out=operonmapper_output,
        email=email
    conda:
        envs / "operonmapper.yaml"
    shell:
        """
        pip install -r {scripts}/operonmapper.requirements.txt
        python {scripts}/operonmapper.py \
            start {input} \
            --email {params.email} \
            --reuse \
            -o {params.out}
        """


rule convert_opfindres_to_gff:
    input:
        txt = operonmapper_output / 'list_of_operons'
    output:
        gff =output_dir / 'operons.gff3'
    conda:
        envs / 'pythonic.yml'
    params:
        script = scripts / 'convert_operonmapper_to_gff3.py',
        seqid = seq_id
    threads:
        1
    log: stderr = logs / "gff_convert.stderr"
    shell:
        """
        (
        python {params.script} --input {input.txt} --output {output.gff} --seqid {params.seqid}
        ) 2> {log.stderr}
        """


rule reinstall_transposones:
    input:
        fna=no_transposone_assembly,
        operons=output_dir / 'operons.gff3',
        rebuild=no_transposone_rebuild_file,
        assembly=input_assembly
    output:
        output_dir / 'operons_reindexed.gff3'
    conda:
        envs / 'pythonic.yml'
    shell:
        """
        python {scripts}/transposon.py rebuild {input.fna} {input.rebuild} \
            --gff {input.operons} \
            --validation {input.assembly} \
            --output {output}
        """


rule find_o_antigen_orfs:
    input:
        faa = rules.operon_mapping.output.pps
    output:
        txt = hmm_results / 'o_ant_products.txt'
    params:
        hmms_path = profiles / 'o_antigen.hmm',
        hmm_thres = config['hmm_threshold']
    conda:
        envs / 'hmmer.yml'
    threads:
        config['maxthreads']
    log: 
        stdout = logs / "hmmsearch.stdout", stderr = logs / "hmmsearch.stderr"
    shell:
        """
        ( hmmsearch --noali --notextw -E {params.hmm_thres} --domE {params.hmm_thres} \
        --tblout {output} {params.hmms_path} {input.faa}
        ) > {log.stdout} 2> {log.stderr} 
        """


rule parse_hmm_res:
    input:
        txt = hmm_results / 'o_ant_products.txt',
    output:
        tsv = hmm_results / 'o_ant_products.tsv'
    params:
        script = scripts / 'parsehmm.py',
        decoder_path = profiles / 'keggs.tsv'
    threads:
        1
    log: 
        stderr = logs / "parsehmm.stderr"
    shell:
        """
        (
        python {params.script} {input.txt} {params.decoder_path} {output.tsv}
        ) 2> {log.stderr}
        """

rule find_operon:
    input:
        tsv = hmm_results /  'o_ant_products.tsv',
        gff = output_dir / 'operons_reindexed.gff3'
    output:
        gff = results / 'o_antigen_operons.gff3'
    params:
        script = scripts / 'findtargetoperon.py',
        keggs_min = config['kegg_minimal']
    threads:
        1
    conda:
        envs / 'pythonic.yml'
    log: 
        stderr = logs / "find_operons.stderr"
    shell:
        """
        ( python {params.script} {input.gff} {input.tsv} {output.gff} {params.keggs_min}) 2> {log.stderr} 
        """


rule extract_operons:
    input:
        gff = results / 'o_antigen_operons.gff3'
    output:
        results / 'operons.tsv'
    conda:
        envs / "pythonic.yml"
    params:
        script = scripts / 'extract_operons_from_operonmapper.py'
    shell:
        """
        pip install genomenotebook > /dev/null
        python {params.script} -i {input.gff} -o {output}
        """


rule remove_fa_tail_gff:
    input:
        bakta_output / f"{input_prefix}.gff3"
    output:
        bakta_output / 'annotation_wo_fa.gff3'  # OTHER DIR ?
    log:
        stderr = logs /'remove_fa_tail.stderr'
    threads:
        1
    shell:
        """
        ( awk '{{if($0 ~ /^##FASTA/) exit; print;}}' {input} > {output} ) 2> {log.stderr} 
        """


rule extract_contig_names:
    input:
        input_assembly
    output:
        bakta_output / 'contig_names.txt'
    log:
        stderr = logs / 'extract_contig_names.stderr'
    threads: 
        1
    shell:
        """
        ( grep '^>' {input} | cut -c 2- | awk '{{print $1}}' > {output} ) 2> {log.stderr} 
        """

rule fuse_annotations:
    input:
        a = results / 'o_antigen_operons.gff3',
        c = bakta_output / 'contig_names.txt',
        b = bakta_output / 'annotation_wo_fa.gff3'
    output:
        results / 'operons_annotation.gff3'
    conda:
        envs / 'pythonic.yml'
    threads:
        1
    log:
        stderr = logs / "fuse_annotation.stderr"
    params:
        script_path = scripts / 'gff_fuse.py'
    shell:
        """
        ( python {params.script_path} --contigs {input.c} --bakta {input.b} \
        --antigens {input.a}  --output {output} ) 2> {log.stderr} 
        """


rule compare_with_reference:
    input:
        ref = reference_gff,
        ant = results / 'operons.tsv',
        gff = results / 'operons_annotation.gff3'
    output:
        results / 'reference_difference.tsv'
    params:
        script =scripts / 'reference_difference.py'
    log:
        stderr = logs / "reference-difference.stderr"
    threads:
        1
    shell:
        """
        ( python {params.script} -r {input.ref} -g {input.gff} -a {input.ant} -o {output} ) 2> {log.stderr}
        """  

        
rule extract_hypos:
    input:
        gff = results / 'operons_annotation.gff3',
        faa = bakta_output / f"{input_prefix}.faa"
    output:
        faa = bakta_output / 'hypothetical_proteins.faa'
    conda:
        envs / 'pythonic.yml'
    threads:
        1
    params:
        script =scripts / 'extract_hypo.py'
    log:
        stderr = "logs/extract_hypo.stderr"
    shell:
        """
         ( python {params.script} {input.gff} {input.faa} {output.faa} ) 2> {log.stderr}
        """

rule plot_operons:
    input:
        gff = results / 'operons_annotation.gff3',
        antigenes = results / 'operons.tsv'
    output:
        results / 'images' / 'finished.txt'
    params:
        out = results / 'images',
        script = scripts / 'plot_operons.py'
    conda:
        envs / 'pythonic.yml'
    shell:
        '''
        pip install genomenotebook > /dev/null
        python {params.script} -g {input.gff} -a {input.antigenes} -o {params.out}
        '''

# rule blast_hypos:
#     input:
#         bakta_output / 'hypothetical_proteins.faa'
#     output:
#          blast_results / 'blast_fmt6.txt'
#     conda:
#         envs / 'blast.yml'
#     threads:
#         maxthreads
#     log:
#         stdout = logs / "blastp.stdout",
#         stderr = logs / "blastp.stderr"
#     shell:
#         """
#         (blastp -query {input} -db nr -task blastp -remote \
#         -outfmt 6 -out {output} -max_target_seqs 6) > {log.stdout} 2> {log.stderr}
#         """

# rule fetch_hypos:
#     input:
#         blast_results / 'blast_fmt6.txt'
#     output:
#         blast_results / 'blast_fetched_names.tsv'
#     conda:
#         envs /'pythonic.yml'
#     log:
#         stderr = logs / "fetch_hypo.stderr"
#     params:
#         email = email,
#         script = scripts / "fetch_hypos_names.py"
#     threads:
#         1
#     shell:
#         """
#         ( python {params.script} {input} {params.email} {output}
#         ) 2> {log.stderr}
#         """

# rule add_hypo_to_gff:
#     input:
#         gff = results / 'operons_annotation.gff3',
#         tsv = blast_results / 'blast_fetched_names.tsv'
#     output:
#         gff = results / 'operons_annotation_with_blast.gff'
#     conda:
#         envs / 'pythonic.yml'
#     threads:
#         1
#     params:
#         script = scripts / 'hypothetical_proteins_fix.py'
#     log:
#         stderr = logs / "add_hypo.stderr"
#     shell:
#         """
#         ( python {params.script} -g {input.gff} -h {input.tsv} -o {output}
#         ) 2> {log.stderr}
#         """
