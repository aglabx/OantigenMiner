from pathlib import Path
import os

data = Path('data_2')
GENOME = "E.coli_genome"
email_ = 'yarevan-hackaton@buft.io'

profiles = Path('hmms')
results = data / Path('results')
hmm_results = results / 'hmm'
scripts = Path('scripts')
logs = Path('logs')

the_results = results / 'final'

os.makedirs(hmm_results, exist_ok=True)
os.makedirs(logs, exist_ok=True)
os.makedirs(the_results, exist_ok=True)


assembly = data / f"{GENOME}.fna"

prodigal_tf = data / "prodigal.trn"
bakta_db = "db-light"
bakta_output = data / "bakta_output"
isescan_output = data / "isescan_output"
annotation = bakta_output / f"{GENOME}.gff3"
proteome = bakta_output / f"{GENOME}.faa"
operonmapper_output = data / "operonmapper_output"

maxthreads = 10
hmm_threshold = 0.0000000000000000001
kegg_minimal = 3
seq_id = 'NC_016854.1' # TODO: READ FROM FNA


# rule all:
#     input:
        
rule gene_boundaries:
    input:
        assembly
    output:
        prodigal_tf
    conda:
        "envs/prodigal.yaml"
    shell:
        "prodigal -i {input} -t {output}"

rule annotate_genomes:
    input:
        assembly=assembly,
        prodigal_tf=prodigal_tf
    output:
        directory(bakta_output)
    conda:
        "envs/bakta.yaml"
    shell:
        """
        ./bakta_loaddb.sh
        bakta {input.assembly} \
            --db {bakta_db} \
            --prodigal-tf {input.prodigal_tf} \
            --output {output}
        """

rule operon_mapping:
    input:
        assembly
    output:
        directory(operonmapper_output)
    params:
        email = email_
    conda:
        "envs/operonmapper.yaml"
    shell:
        """
        pip install -r {scripts}/operonmapper.requirements.txt
        python {scripts}/operonmapper.py \
            start {input} \
            --email {params.email} \
            --reuse \
            -o {output}
        """


rule convert_opfindres_to_gff:
    input:
        txt = operonmapper_output / 'list_of_operons'
    output:
        gff = data / 'operons.gff3'
    conda:
        'envs/pythonic.yml'
    params:
        script_path = scripts / 'convert_operonmapper_to_gff3.py',
        seqid = seq_id
    threads:
        1
    log: stderr = logs / "gff_convert.stderr"
    shell:
        """
        (
        python3 {params.script_path} --input {input.txt} --output {output.gff} --seqid {params.seqid}
        ) 2> {log.stderr}
        """


rule find_o_antigen_orfs:
    input:
        faa = operonmapper_output / 'predicted_protein_sequences'
    output:
        txt = hmm_results / 'o_ant_products.txt'
    params:
        hmms_path = profiles / 'o_antigen.hmm',
        hmm_thres = hmm_threshold
    conda:
        'envs/hmmer.yml'
    threads:
        maxthreads
    log: 
        stdout = logs / "hmmsearch.stdout", stderr = logs / "hmmsearch.stderr"
    shell:
        """
        ( hmmsearch --noali --notextw -E {params.hmm_thres} --domE {params.hmm_thres} \
        --tblout {output} {params.hmms_path} {input.faa}
        ) > {log.stdout} 2> {log.stderr} 
        """


rule parse_hmm_res:
    input:
        txt =  hmm_results / 'o_ant_products.txt',
    output:
        tsv = hmm_results / 'o_ant_products.tsv'
    params:
        scripts_path = scripts / 'parsehmm.py',
        decoder_path = profiles / 'keggs.tsv'
    threads:
        1
    log: 
        stderr = logs / "parsehmm.stderr"
    shell:
        """
        (
        python3 {params.scripts_path} {input.txt} {params.decoder_path} {output.tsv}
        ) 2> {log.stderr}
        """

rule find_operon:
    input:
        tsv = hmm_results /  'o_ant_products.tsv',
        gff = data / 'operons.gff3'
    output:
        gff = the_results / 'o_antigen_operons.gff3'
    params:
        script_path = scripts / 'findtargetoperon.py',
        keggs_min = kegg_minimal
    threads:
        1
    conda:
        'envs/pythonic.yml'
    log: 
        stderr = logs / "find_operons.stderr"
    shell:
        """
        ( python3 {params.script_path} {input.gff} {input.tsv} {output.gff} {params.keggs_min}) 2> {log.stderr} 
        """

rule find_transposones:
    input:
        fna = assembly
    output:
        directory(isescan_output)
    params:
        output_dir = isescan_output
    conda:
        'envs/isescan.yml'
    threads:
        maxthreads
    shell:
        """
        isescan.py --seqfile {input.fna} --output {params.output_dir} --nthread {threads}
        """
rule remove_fa_tail_gff:
    input:
        annotation
    output:
        bakta_output / 'annotation_wo_fa.gff3'  # OTHER DIR ?
    log:
        stderr = logs /'remove_fa_tail.stderr'
    threads:
        1
    shell:
        """
        ( awk '{{if($0 ~ /^##FASTA/) exit; print;}}' {input} > {output} ) 2> {log.stderr} 
        """

rule extract_contig_names:
    input:
        assembly
    output:
        bakta_output / 'cotig_names.txt'
    log:
        stderr = logs / 'extract_contig_names.stderr'
    threads:
        1
    shell:
        """
        ( grep '^>' {input} | cut -c 2- | awk '{{print $1}}' > {output} ) 2> {log.stderr} 
        """

rule fuse_annotations:
    input:
        a = the_results / 'o_antigen_operons.gff3',
        c = bakta_output / 'cotig_names.txt',
        b = bakta_output / 'annotation_wo_fa.gff3'
    output:
        the_results / 'operons_annotation.gff'
    conda:
        'envs/pythonic.yml'
    threads:
        1
    log:
        stderr = "logs/fuse_annotation.stderr"
    params:
        script_path = scripts / 'gff_fuse.py'
    shell:
        """
        ( python3 {params.script_path} --contigs {input.c} --bakta {input.b} \
        --antigens {input.a}  --output {output} ) 2> {log.stderr} 
        """
